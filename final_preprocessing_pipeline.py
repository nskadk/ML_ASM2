# -*- coding: utf-8 -*-
"""final_preprocessing_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wLi0cwk2MAW8a2l9MpJ0CyBsl_kBlOuN
"""

import os
import cv2
import torch
from torch.utils.data import Dataset
import numpy as np
from tqdm import tqdm
import json
import logging
from concurrent.futures import ThreadPoolExecutor
from skimage.measure import shannon_entropy
import pandas as pd

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

###########################################
# Device Management and Base Classes
###########################################

class DeviceManager:
    """Manages device selection for processing"""
    @staticmethod
    def get_device():
        """Returns the best available device (GPU or CPU)"""
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")

class ImageLoader:
    """Abstract class for image loading strategies"""
    def load_image(self, path):
        raise NotImplementedError("Subclasses must implement load_image")

class OpenCVImageLoader(ImageLoader):
    """OpenCV-based image loader"""
    def load_image(self, path):
        """Loads an image using OpenCV"""
        try:
            img = cv2.imread(path)
            if img is None:
                return None, f"{path} (Error: Could not read image)"
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            return img, None
        except Exception as e:
            return None, f"{path} (Error: {str(e)})"

class DuplicateImageDetector:
    """Detects and removes duplicate images using perceptual hashing"""
    def __init__(self, hash_size=8, threshold=5):
        self.hash_size = hash_size
        self.threshold = threshold
        self.hashes = {}  # Stores {image_path: hash}

    def compute_phash(self, image):
        """Compute perceptual hash for image detection"""
        if image is None:
            return None

        # Resize and convert to grayscale
        img = cv2.resize(image, (self.hash_size * 4, self.hash_size * 4))
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

        # Compute DCT and get hash
        dct = cv2.dct(np.float32(gray))
        dct_low = dct[:self.hash_size, :self.hash_size]
        median = np.median(dct_low)
        hash_bits = dct_low > median

        # Convert to integer hash
        hash_value = 0
        for i, bit in enumerate(hash_bits.flatten()):
            hash_value += int(bit) * (2 ** i)
        return hash_value

    def hamming_distance(self, hash1, hash2):
        """Calculate bit difference between hashes"""
        return bin(int(hash1) ^ int(hash2)).count('1')

    def detect_duplicates(self, image_paths, loader):
        """Find duplicate images in dataset"""
        self.hashes = {}
        duplicates = []
        unique_paths = []

        for path in tqdm(image_paths, desc="Detecting duplicates"):
            try:
                img, error = loader.load_image(path)
                if error or img is None:
                    continue

                img_hash = self.compute_phash(img)
                if img_hash is None:
                    continue

                # Check against existing hashes
                is_duplicate = False
                for existing_path, existing_hash in self.hashes.items():
                    if self.hamming_distance(img_hash, existing_hash) <= self.threshold:
                        duplicates.append((existing_path, path))
                        is_duplicate = True
                        break

                if not is_duplicate:
                    self.hashes[path] = img_hash
                    unique_paths.append(path)

            except Exception as e:
                logger.error(f"Error processing {path}: {str(e)}")

        return unique_paths, duplicates

###########################################
# Image Analysis Classes
###########################################

class ImageAnalyzer:
    """Abstract class for image analysis strategies"""
    def analyze_image(self, img, device=None):
        """Analyze an image and return metrics"""
        raise NotImplementedError("Subclasses must implement analyze_image")

class PlantDiseaseAnalyzer(ImageAnalyzer):
    """Analyzer for plant disease images."""
    def __init__(self, device='cpu'):
        self.device = torch.device(device)
        logger.info(f"Initialized PlantDiseaseAnalyzer with device={device}")

    def analyze_image(self, img, img_path):
        """Analyzes a plant disease image and returns statistical metrics."""
        # Check if the image is valid
        if img is None or img.size == 0:
            logger.error(f"Invalid image at {img_path}: Image is None or empty")
            return None, f"{img_path} (Error: Image is None or empty)"

        image_stats = {}
        try:
            # Ensure image is in RGB format
            if len(img.shape) == 2:  # Grayscale to RGB
                img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
            elif img.shape[2] == 4:  # Remove alpha channel if present
                img = img[:, :, :3]

            # Resize and convert to tensor
            img_resized = cv2.resize(img, (224, 224))
            img_tensor = torch.from_numpy(img_resized.transpose(2, 0, 1)).float() / 255.0
            img_tensor = img_tensor.to(self.device)

            # Lighting (mean brightness in grayscale)
            gray_tensor = 0.299 * img_tensor[0] + 0.587 * img_tensor[1] + 0.114 * img_tensor[2]
            image_stats['lighting'] = gray_tensor.mean().item()

            # Contrast (standard deviation of grayscale)
            image_stats['contrast'] = gray_tensor.std().item() * 255.0

            # Color distribution (green channel stats)
            green_channel = img_tensor[1]
            image_stats['color_distribution'] = (green_channel.mean().item(), green_channel.std().item())

            # Convert to grayscale and HSV for further analysis
            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)

            # Noise and blur (Laplacian variance)
            laplacian = cv2.Laplacian(gray, cv2.CV_64F)
            noise_score = laplacian.var()
            image_stats['noise'] = noise_score if np.isfinite(noise_score) else 0.0
            image_stats['blur'] = noise_score if np.isfinite(noise_score) else 0.0

            # Background ratio (plant vs. background using green mask)
            green_mask = cv2.inRange(hsv, (25, 25, 25), (90, 255, 255))
            if green_mask.size > 0:
                plant_ratio = np.sum(green_mask) / (green_mask.shape[0] * green_mask.shape[1] * 255)
                image_stats['background_ratio'] = plant_ratio if np.isfinite(plant_ratio) else 0.0
            else:
                image_stats['background_ratio'] = 0.0

            # Entropy (information content of grayscale image)
            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
            hist = hist / hist.sum()  # Normalize histogram
            entropy_val = shannon_entropy(hist, base=2)
            image_stats['entropy'] = entropy_val if np.isfinite(entropy_val) else 0.0

            return image_stats, None

        except Exception as e:
            logger.error(f"Analysis failed for {img_path}: {str(e)}")
            return None, f"Analysis error for {img_path}: {str(e)}"

class ResultsAggregator:
    """Aggregates and reports analysis results."""
    def __init__(self):
        self.results = {
            'lighting': [], 'contrast': [], 'noise': [], 'blur': [],
            'entropy': [], 'background_ratio': []
        }
        self.errors = []  # To store errors for later review

    def add_result(self, result, error=None):
        """Adds analysis results and errors to the aggregator."""
        # Handle errors if they exist
        if error:
            self.errors.append(error)
            logger.error(error)
        # Process results if they exist
        if result is not None:
            for key in self.results.keys():
                if key in result:
                    self.results[key].append(result[key])

    def generate_report(self):
        """Generates a statistical report from collected data."""
        report = {}
        for key in ['lighting', 'contrast', 'noise', 'blur', 'entropy']:
            data = self.results[key]
            if not data:
                logger.warning(f"No valid data for {key}. Using default values.")
                report[key] = {'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0}
            else:
                report[key] = {
                    'mean': float(np.mean(data)),
                    'std': float(np.std(data)),
                    'min': float(np.min(data)),
                    'max': float(np.max(data))
                }
        if self.results['background_ratio']:
            report['background'] = {
                'mean_plant_ratio': float(np.mean(self.results['background_ratio'])),
                'std_plant_ratio': float(np.std(self.results['background_ratio']))
            }
        else:
            logger.warning("No valid background data. Using default values.")
            report['background'] = {'mean_plant_ratio': 0.0, 'std_plant_ratio': 0.0}
        return report

###########################################
# Image Preprocessing Classes
###########################################

class ImagePreprocessor:
    """Abstract class for image preprocessing strategies"""
    def determine_pipeline(self, dataset_stats):
        """Determine the preprocessing pipeline based on dataset stats"""
        raise NotImplementedError("Subclasses must implement determine_pipeline")

    def preprocess_image(self, img, pipeline, target_size=(224, 224)):
        """Preprocess an image according to the pipeline"""
        raise NotImplementedError("Subclasses must implement preprocess_image")

class PlantDiseasePreprocessor(ImagePreprocessor):
    """Preprocessor for plant disease images"""
    def determine_pipeline(self, dataset_stats):
        """Determine optimal preprocessing pipeline based on statistical analysis"""
        pipeline = []
        pipeline_description = []

        # Default thresholds (could be moved to a configuration)
        thresholds = {
            'lighting': {'range': 80, 'std': 40},
            'contrast': {'mean': 40},
            'noise': {'mean': 300},
            'blur': {'mean': 200},
            'background': {'mean_plant_ratio': 0.5}
        }

        # Extract stats
        lighting_stats = dataset_stats.get('lighting', {})
        contrast_stats = dataset_stats.get('contrast', {})
        noise_stats = dataset_stats.get('noise', {})
        blur_stats = dataset_stats.get('blur', {})
        background_stats = dataset_stats.get('background', {})

        # Determine needs
        needs_lighting_correction = False
        if lighting_stats.get('mean') is not None:
            lighting_range = lighting_stats.get('max', 0) - lighting_stats.get('min', 0)
            lighting_std = lighting_stats.get('std', 0)
            needs_lighting_correction = lighting_range > thresholds['lighting']['range'] or lighting_std > thresholds['lighting']['std']

        needs_contrast_enhancement = False
        if contrast_stats.get('mean') is not None:
            needs_contrast_enhancement = contrast_stats.get('mean', 100) < thresholds['contrast']['mean']

        needs_denoising = False
        if noise_stats.get('mean') is not None:
            needs_denoising = noise_stats.get('mean', 0) > thresholds['noise']['mean']

        needs_sharpening = False
        if blur_stats.get('mean') is not None:
            needs_sharpening = blur_stats.get('mean', 1000) < thresholds['blur']['mean']

        needs_segmentation = True  # Default to yes
        if background_stats.get('mean_plant_ratio') is not None:
            needs_segmentation = background_stats.get('mean_plant_ratio', 0) < thresholds['background']['mean_plant_ratio']

        # 1. Always start with resizing for consistency
        pipeline.append("resize")
        pipeline_description.append("Resize images to consistent dimensions")

        # 2. Background removal (if needed)
        if needs_segmentation:
            pipeline.append("segmentation")
            pipeline_description.append("Remove background to isolate plant tissue")

        # 3. Denoising (if needed)
        if needs_denoising:
            pipeline.append("denoise")
            pipeline_description.append("Apply denoising to reduce image noise")

        # 4. Lighting correction (if needed)
        if needs_lighting_correction:
            pipeline.append("lighting_correction")
            pipeline_description.append("Correct uneven lighting across dataset")

        # 5. Contrast enhancement (if needed)
        if needs_contrast_enhancement:
            pipeline.append("contrast_enhancement")
            pipeline_description.append("Enhance contrast to highlight disease features")

        # 6. Sharpening (if needed)
        if needs_sharpening:
            pipeline.append("sharpen")
            pipeline_description.append("Apply sharpening to enhance disease boundaries")

        # 7. Color space transformations
        pipeline.append("color_transformation")
        pipeline_description.append("Transform to disease-relevant color spaces (HSV, LAB)")

        # 8. Finish with normalization for model training
        pipeline.append("normalize")
        pipeline_description.append("Normalize pixel values for model training")

        return pipeline, pipeline_description

    def preprocess_image(self, img, pipeline, target_size=(224, 224)):
        """Apply preprocessing pipeline to a single image with enhanced statistical handling"""
        if img is None or img.size == 0:
            logger.error("Cannot preprocess empty or null image")
            return None

        # Make a copy to avoid modifying original
        processed = img.copy()

        # Get image statistics for adaptive processing
        try:
            # Initial statistics in RGB
            rgb_stats = {
                'mean': np.mean(processed, axis=(0,1)),
                'std': np.std(processed, axis=(0,1)),
                'min': np.min(processed, axis=(0,1)),
                'max': np.max(processed, axis=(0,1))
            }

            # Convert to LAB for perceptual analysis
            if len(processed.shape) == 3 and processed.shape[2] == 3:
                lab = cv2.cvtColor(processed, cv2.COLOR_RGB2LAB)
                l_channel = lab[:,:,0]
                lab_stats = {
                    'l_mean': np.mean(l_channel),
                    'l_std': np.std(l_channel),
                    'l_min': np.min(l_channel),
                    'l_max': np.max(l_channel)
                }
            else:
                lab_stats = {'l_mean': 128, 'l_std': 20, 'l_min': 0, 'l_max': 255}

            # Analyze edges and texture for adaptive sharpening
            if len(processed.shape) >= 2:
                gray = cv2.cvtColor(processed, cv2.COLOR_RGB2GRAY) if len(processed.shape) == 3 else processed
                edges = cv2.Canny(gray, 100, 200)
                texture_stats = {
                    'edge_density': np.sum(edges > 0) / edges.size,
                    'entropy': shannon_entropy(gray.ravel() / 255.0, base=2) if 'shannon_entropy' in globals() else 4.0
                }
            else:
                texture_stats = {'edge_density': 0.1, 'entropy': 4.0}

        except Exception as e:
            logger.warning(f"Error calculating image statistics: {str(e)}. Using defaults.")
            rgb_stats = {'mean': np.array([128, 128, 128]), 'std': np.array([50, 50, 50]),
                        'min': np.array([0, 0, 0]), 'max': np.array([255, 255, 255])}
            lab_stats = {'l_mean': 128, 'l_std': 20, 'l_min': 0, 'l_max': 255}
            texture_stats = {'edge_density': 0.1, 'entropy': 4.0}

        # Apply each preprocessing step with adaptive parameters
        for step in pipeline:
            try:
                if step == "resize":
                    # Use INTER_AREA for downsampling, INTER_CUBIC for upsampling
                    h, w = processed.shape[:2]
                    if h > target_size[1] or w > target_size[0]:
                        # Downsampling - use INTER_AREA for better quality
                        processed = cv2.resize(processed, target_size, interpolation=cv2.INTER_AREA)
                    else:
                        # Upsampling - use INTER_CUBIC for better quality
                        processed = cv2.resize(processed, target_size, interpolation=cv2.INTER_CUBIC)

                elif step == "segmentation":
                    # Convert to HSV for better plant segmentation
                    hsv = cv2.cvtColor(processed, cv2.COLOR_RGB2HSV)

                    # Adaptive green range based on image statistics
                    # Calculate mean hue to center the green range
                    mean_hue = np.mean(hsv[:,:,0])
                    green_center = 60  # Standard green is around 60 in OpenCV HSV

                    # If the image has a color cast, adjust the green center
                    if abs(mean_hue - green_center) > 15:
                        green_center = (mean_hue + green_center) / 2

                    green_range = 30  # Span around the green center
                    lower_green = np.array([max(0, green_center - green_range), 30, 30])
                    upper_green = np.array([min(180, green_center + green_range), 255, 255])

                    # Yellow-brown range for many common diseases
                    # Adaptive based on image characteristics
                    if rgb_stats['mean'][0] > rgb_stats['mean'][1]:  # Reddish tones dominate
                        lower_disease = np.array([5, 40, 40])  # More toward red
                        upper_disease = np.array([25, 255, 255])
                    else:  # Yellowish tones dominate
                        lower_disease = np.array([15, 40, 40])
                        upper_disease = np.array([35, 255, 255])

                    # Create masks
                    mask_plant = cv2.inRange(hsv, lower_green, upper_green)
                    mask_disease = cv2.inRange(hsv, lower_disease, upper_disease)

                    # Combined mask
                    mask = cv2.bitwise_or(mask_plant, mask_disease)

                    # Determine kernel size based on image size
                    kernel_size = max(3, min(9, int(min(processed.shape[:2]) / 50)))
                    kernel = np.ones((kernel_size, kernel_size), np.uint8)

                    # Apply morphological operations
                    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
                    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

                    # Apply mask only if it's not empty
                    if np.sum(mask) > 0.05 * mask.size:  # At least 5% of pixels
                        # Create a binary 3-channel mask
                        mask_3d = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB) / 255.0
                        # Apply mask with fading alpha to maintain some background context
                        processed = cv2.addWeighted(
                            processed, 1.0,
                            (processed * mask_3d).astype(np.uint8), 0.0,
                            0
                        )
                        # Fill background with mild gray to avoid black
                        bg_mask = 1 - mask_3d
                        bg_color = np.ones_like(processed) * 245  # Light gray
                        processed = processed + (bg_color * bg_mask).astype(np.uint8)

                elif step == "denoise":
                    # Adaptive denoising based on image noise level
                    # Check if image is very noisy
                    gray = cv2.cvtColor(processed, cv2.COLOR_RGB2GRAY) if len(processed.shape) == 3 else processed
                    noise_metric = np.std(cv2.Laplacian(gray, cv2.CV_64F)) / 10.0

                    # Adaptive parameters based on noise level
                    h_luminance = max(5, min(15, int(noise_metric)))
                    h_color = max(5, min(15, int(noise_metric * 1.5)))

                    # Apply Non-Local Means denoising with adaptive parameters
                    if noise_metric > 2.0:  # Only denoise if there's significant noise
                        processed = cv2.fastNlMeansDenoisingColored(
                            processed, None,
                            h_luminance, h_color,
                            7, 21  # Template window and search window sizes
                        )

                elif step == "lighting_correction":
                    # Convert to LAB color space
                    lab = cv2.cvtColor(processed, cv2.COLOR_RGB2LAB)
                    l, a, b = cv2.split(lab)

                    # Adaptive CLAHE based on image contrast
                    if lab_stats['l_std'] < 30:  # Low contrast image
                        clip_limit = 3.0
                        grid_size = (8, 8)
                    elif lab_stats['l_std'] < 50:  # Medium contrast
                        clip_limit = 2.0
                        grid_size = (8, 8)
                    else:  # High contrast already
                        clip_limit = 1.5
                        grid_size = (6, 6)

                    # Apply CLAHE to L channel
                    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)
                    cl = clahe.apply(l)

                    # Rescale L channel to maintain natural look
                    l_min, l_max = lab_stats['l_min'], lab_stats['l_max']
                    cl_range = np.max(cl) - np.min(cl)
                    if cl_range > 0:
                        cl = np.clip(((cl - np.min(cl)) / cl_range) * (l_max - l_min) + l_min, 0, 255).astype(np.uint8)

                    # Merge channels and convert back to RGB
                    processed = cv2.cvtColor(cv2.merge((cl, a, b)), cv2.COLOR_LAB2RGB)

                elif step == "contrast_enhancement":
                    # Adaptive contrast enhancement based on image statistics
                    if lab_stats['l_std'] < 20:  # Very low contrast
                        alpha = 1.5  # Stronger enhancement
                        beta = 15
                    elif lab_stats['l_std'] < 40:  # Low contrast
                        alpha = 1.3
                        beta = 10
                    elif lab_stats['l_std'] < 60:  # Medium contrast
                        alpha = 1.1
                        beta = 5
                    else:  # Good contrast already
                        alpha = 1.0
                        beta = 0

                    # Apply contrast enhancement only if needed
                    if alpha > 1.0 or beta > 0:
                        # Clip to avoid overflow
                        processed = np.clip(cv2.convertScaleAbs(processed, alpha=alpha, beta=beta), 0, 255)

                elif step == "sharpen":
                    # Determine if sharpening is needed based on edge density
                    if texture_stats['edge_density'] < 0.05:  # Few edges, likely blurry
                        # Stronger sharpening for blurry images
                        kernel = np.array([[-1, -1, -1],
                                          [-1, 9.5, -1],
                                          [-1, -1, -1]])
                    elif texture_stats['edge_density'] < 0.1:  # Moderate edges
                        # Moderate sharpening
                        kernel = np.array([[-0.5, -1, -0.5],
                                          [-1, 7, -1],
                                          [-0.5, -1, -0.5]])
                    else:  # Many edges already, gentle sharpening
                        kernel = np.array([[-0.25, -0.5, -0.25],
                                          [-0.5, 4.5, -0.5],
                                          [-0.25, -0.5, -0.25]])

                    # Apply sharpening
                    processed = cv2.filter2D(processed, -1, kernel)

                    # Make sure image is still in valid range
                    processed = np.clip(processed, 0, 255).astype(np.uint8)

                elif step == "color_transformation":
                    # Enhance colors for better disease visibility
                    # This depends on disease types, but generally:
                    # 1. Enhance green-yellow separation
                    # 2. Enhance saturation for spots and lesions

                    # Convert to HSV for color manipulation
                    hsv = cv2.cvtColor(processed, cv2.COLOR_RGB2HSV)
                    h, s, v = cv2.split(hsv)

                    # Enhance saturation slightly to make spots more visible
                    # More enhancement for low-saturation images
                    mean_sat = np.mean(s)
                    if mean_sat < 50:
                        s = np.clip(s * 1.4, 0, 255).astype(np.uint8)
                    elif mean_sat < 100:
                        s = np.clip(s * 1.2, 0, 255).astype(np.uint8)
                    else:
                        s = np.clip(s * 1.1, 0, 255).astype(np.uint8)

                    # Recombine and convert back to RGB
                    processed = cv2.cvtColor(cv2.merge((h, s, v)), cv2.COLOR_HSV2RGB)

                elif step == "normalize":
                    # Calculate per-channel mean and std for normalization
                    mean = rgb_stats['mean'] / 255.0
                    std = rgb_stats['std'] / 255.0
                    std = np.where(std < 0.01, 0.01, std)  # Avoid division by zero

                    # Convert to float32 for normalization
                    processed = processed.astype(np.float32) / 255.0

                    # Normalize each channel separately
                    for i in range(3):
                        processed[:,:,i] = (processed[:,:,i] - mean[i]) / std[i]

                    # Rescale to 0-255 and convert back to uint8 for visualization
                    processed = np.clip(processed * 64 + 128, 0, 255).astype(np.uint8)

            except Exception as e:
                logger.warning(f"Error in preprocessing step '{step}': {str(e)}. Skipping this step.")
                continue

        return processed

###########################################
# Dataset Classes
###########################################

class PlantDiseaseDataset(Dataset):
    """Dataset class for plant disease images"""
    def __init__(self, image_paths=None, transform=None, target_size=(224, 224)):
        self.image_paths = []
        self.labels = []
        self.classes = []
        self.transform = transform
        self.target_size = target_size

        if image_paths:
            if isinstance(image_paths, list):
                self.image_paths = image_paths
            else:
                # Assume it's a directory
                base_path = image_paths
                for root, _, files in os.walk(image_paths):
                    if root == base_path:
                        continue
                    # Get the relative path from the dataset root
                    rel_path = os.path.relpath(root, base_path)
                    # Extract class name (the first directory level after the base path)
                    class_name = rel_path.split(os.sep)[0] if os.sep in rel_path else rel_path
                    if class_name not in self.classes and len(files) > 0:
                        self.classes.append(class_name)

                    for file in files:
                        if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                            path = os.path.join(root, file)
                            self.image_paths.append(path)
                            self.labels.append(class_name)

                # Create class to index mapping
                self.class_to_idx = {cls: i for i, cls in enumerate(sorted(self.classes))}
                # Map labels to indices
                self.label_indices = [self.class_to_idx.get(label, 0) for label in self.labels]

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]

        # Load image
        try:
            img = cv2.imread(img_path)
            if img is None:
                # Return a blank image if loading fails
                img = np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        except:
            img = np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8)

        # Apply transforms if any
        if self.transform:
            img = self.transform(img)

        # Get label if available
        if hasattr(self, 'label_indices') and len(self.label_indices) > idx:
            label = self.label_indices[idx]
            return img, label

        return img

    def get_class_weights(self):
        """Calculate class weights for imbalanced datasets"""
        if not hasattr(self, 'label_indices') or len(self.label_indices) == 0:
            return None

        class_counts = np.bincount(self.label_indices)
        total_samples = float(len(self.label_indices))
        weights = total_samples / (len(class_counts) * class_counts)
        return weights

    def get_classes(self):
        """Return list of class names"""
        return self.classes

class TrainingDataset(Dataset):
    """Custom dataset for preprocessed plant disease images"""
    def __init__(self, root_dir, transform=None):
        """
        Args:
            root_dir (str): Directory with preprocessed images
            transform (callable, optional): Transform to apply to images
        """
        self.root_dir = root_dir
        self.transform = transform
        self.classes = []
        self.class_to_idx = {}
        self.samples = []  # (path, class_idx) tuples

        # Load class info
        self._find_classes()
        self._make_dataset()

        logger.info(f"Loaded dataset with {len(self.samples)} images across {len(self.classes)} classes")

    def _find_classes(self):
        """Find class folders in the dataset directory"""
        classes = [d.name for d in os.scandir(self.root_dir) if d.is_dir()]
        classes.sort()
        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}
        self.classes = classes

    def _make_dataset(self):
        """Create a list of (path, class_idx) tuples"""
        for target_class in sorted(self.class_to_idx.keys()):
            class_idx = self.class_to_idx[target_class]
            class_dir = os.path.join(self.root_dir, target_class)

            for root, _, fnames in sorted(os.walk(class_dir, followlinks=True)):
                for fname in sorted(fnames):
                    if self._is_image_file(fname):
                        path = os.path.join(root, fname)
                        self.samples.append((path, class_idx))

    def _is_image_file(self, filename):
        """Check if a file is an image"""
        return filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        """Get image and label at index"""
        path, target = self.samples[idx]

        # Load image
        img = cv2.imread(path)
        if img is None:
            # Return a blank image if loading fails
            img = np.zeros((224, 224, 3), dtype=np.uint8)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Apply transforms if any
        if self.transform:
            img = self.transform(img)

        return img, target

    def get_class_weights(self):
        """Calculate class weights for imbalanced datasets"""
        # Count samples per class
        class_counts = {}
        for _, class_idx in self.samples:
            if class_idx not in class_counts:
                class_counts[class_idx] = 0
            class_counts[class_idx] += 1

        # Calculate weights
        total_samples = float(len(self.samples))
        weights = {cls: total_samples / (len(class_counts) * count)
                 for cls, count in class_counts.items()}

        return weights

###########################################
# Pipeline Classes
###########################################
class PreprocessingPipeline:
    """Pipeline for preprocessing plant disease images"""
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.image_loader = OpenCVImageLoader()
        self.analyzer = PlantDiseaseAnalyzer(self.device)
        self.preprocessor = PlantDiseasePreprocessor()
        self.aggregator = ResultsAggregator()
        self.duplicate_detector = DuplicateImageDetector(hash_size=8, threshold=5)

        self.dataset = None
        self.pipeline = None
        self.pipeline_description = None
        self.dataset_report = None
        self.processed_images = []
        self.processed_labels = []
        self.classes = []

        # Create output directories
        os.makedirs(config['output_dir'], exist_ok=True)
        os.makedirs(os.path.join(config['output_dir'], 'preprocessed'), exist_ok=True)
        os.makedirs(os.path.join(config['output_dir'], 'analysis'), exist_ok=True)

        logger.info(f"Initialized preprocessing pipeline with config: {json.dumps(config, indent=2)}")

    def run(self):
        """Execute the preprocessing pipeline"""
        logger.info("Starting preprocessing pipeline...")

        # Step 1: Analyze dataset
        logger.info("Step 1/4: Analyzing dataset...")
        self._analyze_dataset()

        # Step 2: Detect and remove duplicates
        logger.info("Step 2/4: Detecting and removing duplicates...")
        self._remove_duplicates()

        # Step 3: Determine preprocessing pipeline
        logger.info("Step 3/4: Determining optimal preprocessing pipeline...")
        self._determine_pipeline()

        # Step 4: Preprocess images
        logger.info("Step 4/4: Preprocessing images...")
        self.preprocess_folder()

        # Check if we have any processed images
        if not self.processed_images or len(self.processed_images) == 0:
            logger.error("No images were successfully processed. Check earlier pipeline steps.")
            # Create minimal dataset structure to allow pipeline to complete
            for class_name in self.classes:
                os.makedirs(os.path.join(self.config['output_dir'], 'preprocessed', class_name), exist_ok=True)

            # Create minimal dataset info
            self._save_pipeline_info("No images were successfully processed")
            logger.info("Created minimal dataset structure due to processing errors")
            return os.path.join(self.config['output_dir'], 'pipeline_info.json')

        # Save pipeline info
        self._save_pipeline_info()

        logger.info("Preprocessing pipeline completed!")
        return os.path.join(self.config['output_dir'], 'pipeline_info.json')

    def _remove_duplicates(self):
        """Detect and remove duplicate images from dataset"""
        if not self.dataset or len(self.dataset.image_paths) == 0:
            logger.warning("No images to check for duplicates")
            self.duplicates_removed = 0
            return

        logger.info(f"Checking {len(self.dataset.image_paths)} images for duplicates...")

        # Detect duplicates
        unique_paths, duplicates = self.duplicate_detector.detect_duplicates(
            self.dataset.image_paths, self.image_loader)

        # Log results
        num_duplicates = len(duplicates)
        logger.info(f"Found {num_duplicates} duplicate images out of {len(self.dataset.image_paths)} total images")

        if num_duplicates > 0:
            # Save list of duplicates for reference
            duplicates_file = os.path.join(self.config['output_dir'], 'analysis', 'duplicates.json')
            with open(duplicates_file, 'w') as f:
                json.dump([{"original": orig, "duplicate": dup} for orig, dup in duplicates], f, indent=2)
            logger.info(f"Saved duplicate information to {duplicates_file}")

            # Update dataset with only unique images
            # Create mapping of old paths to indexes
            path_to_idx = {path: idx for idx, path in enumerate(self.dataset.image_paths)}

            # Update image paths and labels
            original_count = len(self.dataset.image_paths)
            self.dataset.image_paths = unique_paths

            if hasattr(self.dataset, 'labels') and len(self.dataset.labels) > 0:
                self.dataset.labels = [self.dataset.labels[path_to_idx[path]] for path in unique_paths]
                if hasattr(self.dataset, 'class_to_idx') and self.dataset.class_to_idx:
                    self.dataset.label_indices = [self.dataset.class_to_idx[label] for label in self.dataset.labels]

            logger.info(f"Updated dataset: removed {original_count - len(unique_paths)} duplicate images")
            self.duplicates_removed = num_duplicates
        else:
            self.duplicates_removed = 0

    def _analyze_dataset(self):
        """Analyze the dataset to gather statistics"""
        self.dataset = PlantDiseaseDataset(self.config['dataset_path'])
        self.classes = self.dataset.classes

        sample_size = min(self.config.get('analysis_sample_size', 100), len(self.dataset))
        indices = np.random.choice(len(self.dataset), sample_size, replace=False)

        with ThreadPoolExecutor(max_workers=self.config.get('num_workers', 4)) as executor:
            futures = [executor.submit(self._analyze_single_image, self.dataset[idx][0],
                                      self.dataset.image_paths[idx]) for idx in indices]

            for future in tqdm(futures, total=sample_size, desc="Analyzing samples"):
                result, error = future.result()
                if error:
                    logger.error(error)
                self.aggregator.add_result(result, error)

        self.dataset_report = self.aggregator.generate_report()

        # Save analysis report
        with open(os.path.join(self.config['output_dir'], 'analysis', 'analysis_report.json'), 'w') as f:
            json.dump(self.dataset_report, f, indent=2)

    def _analyze_single_image(self, img, img_path):
        """Analyze a single image using the analyzer"""
        if img is None or img.size == 0:
            return None, f"{img_path} (Error: Image is None or empty)"
        return self.analyzer.analyze_image(img, img_path)

    def _determine_pipeline(self):
        """Determine the optimal preprocessing pipeline based on analysis"""
        self.pipeline, self.pipeline_description = self.preprocessor.determine_pipeline(self.dataset_report)

        # Save pipeline info
        with open(os.path.join(self.config['output_dir'], 'analysis', 'pipeline_info.json'), 'w') as f:
            json.dump({'pipeline': self.pipeline, 'description': self.pipeline_description}, f, indent=2)

    def preprocess_folder(self, input_folder=None, output_folder=None, recursive=True, maintain_structure=True):
        """Preprocess all images in a folder using the current pipeline

        Args:
            input_folder (str): Path to input folder (uses cleaned dataset if None)
            output_folder (str): Output directory (uses config output_dir if None)
            recursive (bool): Process subfolders recursively (default: True)
            maintain_structure (bool): Preserve folder structure (default: True)

        Returns:
            Dict with processing statistics
        """
        # Determine paths to use
        input_folder = input_folder or self.config['dataset_path']
        output_folder = output_folder or self.config['output_dir']

        # Create output directory
        os.makedirs(output_folder, exist_ok=True)

        # Get appropriate image paths
        if input_folder == self.config['dataset_path']:
            # Use cleaned dataset paths
            image_paths = self.dataset.image_paths
        else:
            # Find images in specified input folder
            image_paths = []
            scan_method = os.walk(input_folder) if recursive else [next(os.walk(input_folder))]

            for root, _, files in scan_method:
                for file in files:
                    if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                        image_paths.append(os.path.join(root, file))

        if not image_paths:
            logger.warning(f"No images found in {input_folder}")
            return {"error": "No images found", "processed": 0, "failed": 0}

        logger.info(f"Processing {len(image_paths)} images from {input_folder}")

        processed_count = 0
        failed_count = 0

        with ThreadPoolExecutor(max_workers=self.config.get('num_workers', 4)) as executor:
            futures = []

            for img_path in image_paths:
                # Determine output path
                if maintain_structure:
                    rel_path = os.path.relpath(os.path.dirname(img_path), input_folder)
                    out_dir = os.path.join(output_folder, rel_path)
                    os.makedirs(out_dir, exist_ok=True)
                else:
                    out_dir = output_folder

                out_path = os.path.join(out_dir, os.path.basename(img_path))
                futures.append(executor.submit(self._process_and_save, img_path, out_path))

            for future in tqdm(futures, total=len(futures), desc="Preprocessing images"):
                success, error = future.result()
                if success:
                    processed_count += 1
                else:
                    failed_count += 1
                    logger.error(error)

        # Update processed images list for pipeline info
        if input_folder == self.config['dataset_path']:
            self.processed_images = image_paths

        report = {
            "total": len(image_paths),
            "processed": processed_count,
            "failed": failed_count,
            "input_folder": input_folder,
            "output_folder": output_folder,
            "pipeline": self.pipeline
        }

        logger.info(f"Folder processing complete. Processed {processed_count} images, failed {failed_count} images.")
        return report

    def _analyze_path(self, img_path):
        """Analyze a single image from path"""
        img, error = self.image_loader.load_image(img_path)
        if error or img is None:
            return None, f"Failed to load {img_path}: {error}"
        return self.analyzer.analyze_image(img, img_path)

    def _process_and_save(self, img_path, out_path):
        """Process a single image and save it"""
        try:
            # Load the image
            img, error = self.image_loader.load_image(img_path)
            if error or img is None:
                return False, f"Failed to load {img_path}: {error}"

            # Process the image
            processed = self.preprocessor.preprocess_image(img, self.pipeline, self.config['target_size'])
            if processed is None:
                return False, f"Failed to process {img_path}"

            # Save processed image
            cv2.imwrite(out_path, cv2.cvtColor(processed, cv2.COLOR_RGB2BGR))
            return True, None

        except Exception as e:
            return False, f"Error processing {img_path}: {str(e)}"

    def preprocess_single_image(self, image_path, output_path=None):
        """Preprocess a single image using the current pipeline

        Args:
            image_path: Path to image file
            output_path: Path to save the processed image (optional)

        Returns:
            Preprocessed image or None if failed
        """
        # Load the image
        img, error = self.image_loader.load_image(image_path)
        if error:
            logger.error(f"Failed to load image: {error}")
            return None

        # Check if pipeline exists, if not, analyze this image to create one
        if self.pipeline is None:
            logger.info("Pipeline not initialized. Analyzing image to determine preprocessing steps...")

            # Analyze the image
            result, error = self.analyzer.analyze_image(img, image_path)
            if error:
                logger.error(f"Failed to analyze image: {error}")
                return None

            # Create minimal stats for determining pipeline
            temp_aggregator = ResultsAggregator()
            temp_aggregator.add_result(result)
            image_stats = temp_aggregator.generate_report()

            # Determine pipeline based on this single image
            self.pipeline, self.pipeline_description = self.preprocessor.determine_pipeline(image_stats)
            logger.info(f"Analysis complete. Determined pipeline: {', '.join(self.pipeline)}")

        try:
            # Apply the preprocessing pipeline
            processed = self.preprocessor.preprocess_image(img, self.pipeline, self.config['target_size'])

            # Save the processed image if output_path is provided
            if output_path and processed is not None:
                output_dir = os.path.dirname(output_path)
                if output_dir:
                    os.makedirs(output_dir, exist_ok=True)
                cv2.imwrite(output_path, cv2.cvtColor(processed, cv2.COLOR_RGB2BGR))
                logger.info(f"Saved processed image to {output_path}")

            return processed
        except Exception as e:
            logger.error(f"Error preprocessing image: {str(e)}")
            return None

    def _save_pipeline_info(self, error=None):
        """Save information about the preprocessing pipeline"""
        class_counts = {}
        for class_name in self.classes:
            class_dir = os.path.join(self.config['output_dir'], 'preprocessed', class_name)
            if os.path.exists(class_dir):
                class_counts[class_name] = len([f for f in os.listdir(class_dir)
                                            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])
            else:
                class_counts[class_name] = 0

        pipeline_info = {
            'classes': self.classes,
            'class_counts': class_counts,
            'preprocessing_pipeline': self.pipeline,
            'pipeline_description': self.pipeline_description,
            'total_images': {
                'original': len(self.dataset),
                'preprocessed': sum(class_counts.values())
            },
            "duplicates": {
                'total_found': getattr(self, 'duplicates_removed', 0)
            },
            'config': self.config
        }

        if error:
            pipeline_info['error'] = error

        with open(os.path.join(self.config['output_dir'], 'pipeline_info.json'), 'w') as f:
            json.dump(pipeline_info, f, indent=2)

###########################################
# Configuration Functions
###########################################
def create_preprocessing_config(dataset_path='./train_images', output_dir='./preprocessed_dataset'):
    """Create configuration for preprocessing pipeline"""
    # Determine CPU count for parallel processing

    return {
        # Basic paths
        'dataset_path': dataset_path,
        'output_dir': output_dir,
        # Processing settings
        'target_size': (224, 224),  # Standard size for most models
        'num_workers': 16,  # Leave some CPU cores free
        'analysis_sample_size': 3000 ,  # Number of images to analyze for statistics
    }

# Define paths
dataset_path = './train_images'  # Path to your original dataset
preprocessed_dir = './processed_dataset_train_testing'  # Where preprocessed images will be saved

# Step 1: Preprocess the dataset
logger.info("=== Starting Preprocessing Pipeline ===")

# # Create preprocessing configuration
preprocess_config = create_preprocessing_config(
    dataset_path=dataset_path,
    output_dir=preprocessed_dir
)

# # Run preprocessing pipeline
preprocessing_pipeline = PreprocessingPipeline(preprocess_config)
# pipeline_info_path = preprocessing_pipeline.run()

# logger.info(f"Preprocessing complete. Results saved to {pipeline_info_path}")

# # Then preprocess a folder of new images using the established pipeline
# input_folder = "./test_images"
# output_folder = "./preprocessed_test_images"
# results = preprocessing_pipeline.preprocess_folder(input_folder, output_folder)

# print(f"Successfully processed {results['processed']} out of {results['total']} images")

import matplotlib.pyplot as plt

# Preprocess the image
processed_img = preprocessing_pipeline.preprocess_single_image("./train_images/bacterial_leaf_blight/100049.jpg", "preprocessed_image.jpg")

if processed_img is not None:
    # Create a figure with two subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Load and show original image
    original_img = cv2.cvtColor(cv2.imread("./train_images/bacterial_leaf_blight/100049.jpg"), cv2.COLOR_BGR2RGB)
    ax1.imshow(original_img)
    ax1.set_title('Original Image')
    ax1.axis('off')

    # Show preprocessed image
    ax2.imshow(processed_img)
    ax2.set_title('Preprocessed Image')
    ax2.axis('off')

    plt.show()
else:
    print("Failed to preprocess image")